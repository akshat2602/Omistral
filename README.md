# OMistral

An Ollama-style inference system with pluggable backends (vLLM, mistral.rs, llama.cpp).

## Features
- C++ inference engine core with Rust API/CLI
- Multiple backend support