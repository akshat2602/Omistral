# OMistral

An Ollama-style inference system with pluggable backends (vLLM, mistral.rs, llama.cpp).

